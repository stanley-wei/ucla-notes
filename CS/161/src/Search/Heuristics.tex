\documentclass[12pt]{extarticle}
\usepackage[export]{adjustbox}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{colortbl}
\usepackage{fancyhdr}
\usepackage[lmargin=0.9in,rmargin=0.9in,bmargin=1.25in]{geometry}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{subfiles}
\usepackage[most]{tcolorbox}
\usepackage[explicit]{titlesec}
\usepackage{ulem}

\graphicspath{ {./../Images/Notes/} }

\title{CS161: Fundamentals of Artificial Intelligence}
\author{Stanley Wei}
\date{Prof. van den Broeck $\vert$ Winter 2024}

\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{#1\hrule\vspace*{-14pt}}

\titleformat{\subsubsection}
  {\normalfont\bfseries}{}{0pt}{\uline{#1}}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\newtheorem*{observation}{Observation}

\theoremstyle{remark}
\newtheorem*{example}{Ex}
\newtheorem*{note}{($\ast$) Note}

\newcommand{\pstart}[0]{\noindent}
\newcommand{\newp}[0]{~\\ \pstart}
\newcommand{\term}[1]{\noindent\textbf{\textit{#1}}}
\newcommand{\titleul}[1]{\noindent \textbf{\ul{#1}}}
\newcommand{\claim}[1]{\noindent Claim: \textit{#1}}
\newcommand{\resetcases}[0]{\setcounter{case}{0}}

\newcommand{\prob}[1]{\text{Pr}(#1)}
\newcommand{\cond}[2]{#1\,\vert\,#2}

\begin{document}
\subsection{Informed Search}
In the case of \term{informed search}, we may be able to use problem-specific domain knowledge to help find solutions faster; we can represent our knowledge as a \term{heuristic function} $h(n)$. 

\newp
A heuristic typically represents an \textit{educated guess} about which states are best/most likely to lead to a solution/an optimal solution.
\begin{itemize}
    \item \textit{Trivial heuristic}: $h(n)=0\;\forall\,n$ is equivalent to doing a blind search
    \item[($\ast$)] \textit{Ex}: A heuristic may represent a distance from a goal state (with $h(n_{goal})=0$)
\end{itemize}

~\\
\term{Greedy/best-first search}: Taking cost function $g(n)=h(n)$, always expand the frontier node with lowest cost. \begin{itemize}
    \item[($+$)] Generally very fast
    \item[($-$)] Not optimal; is complete \ul{only on finite state spaces}
    \item[($-$)] Worst-case time/space complexity is the same as DFS (generally better in practice)
\end{itemize}


% A* Search
\subsubsection{A* Search}
\textit{Insight}: We can make a new algorithm by combining greedy search and uniform-cost search. \begin{itemize}
    \item Greedy search by itself is too greedy (i.e. no memory)
    \item Uniform-cost is too conservative (no knowledge of goal)
\end{itemize}

~\\ \pstart
\textit{Idea}: Given a node $n$, we can estimate the cost $f(n)$ of a solution through $n$ in terms of: \begin{enumerate}
    \item Distance from initial state $g(n)$
    \item Estimated distance from goal $h(n)$
\end{enumerate}
\begin{center}
    \noindent $\implies$ \titleul{Estimated cost}: $f(n)=g(n)+h(n)$
\end{center}

\vspace{8pt}
\term{A$^\ast$ Search}: Taking $f(n)$ as above, run \ul{uniform-cost search}. \begin{itemize}
    \item Every time a node is added to frontier, delete all copies with worse cost
\end{itemize}

\begin{definition}[\term{Admissible Heuristic}] A heuristic is \term{admissible} if it never overestimates the true cost of reaching the goal. \begin{itemize}
    \item Must be overly optimistic - assume best-case scenario
\end{itemize}
\end{definition}

~\\ \pstart
For A$^\ast$ search, want heuristics to be (1) \ul{non-negative} and (2) \ul{\textbf{admissible}}.

~\\ \pstart
\titleul{Properties of A$^\ast$}: 
\begin{enumerate}
    \item \textbf{Is optimal} \ul{if heuristic is admissible}
    \item \textbf{Is complete}: Same argument as uniform-cost
    \item \textbf{Time/Space Complexity}: Same as uniform-cost
\end{enumerate}

\newp
\titleul{Proof of Optimality}
\begin{proof}
    Let $x$ be the last non-goal node in an optimal solution; suffices to show that the goal will never be expanded before $x$ (algorithm will never terminate before expanding $x$).
    
    \newp
    Comparing costs for each node: \begin{enumerate}
        \item Goal: $f(goal)=g(goal)+h(goal)=g(goal)$
        \item $x$: $f(x)=g(x)+h(x)$
    \end{enumerate}
    
    \newp 
    Assuming the goal was generated by a non-optimal solution, then $g(goal)>c^\ast$.
    
    \newp
    Assuming our heuristic $h(n)$ is admissible, then $f(x)=g(x)+h(x)\leq c^\ast$.
    
    \newp
    Then $f(x)<f(goal)$, therefore $x$ will always be expanded before the goal.
\end{proof}

\vspace{8pt}
\newp
Can also define \term{consistent heuristics}: a heuristic $h$ such that $h(n)$ is less than or equal to the cost of any neighbor $h(n')$ plus the cost of reaching that neighbor

\vspace{5pt}
\pstart$\implies$ \term{graph search A$^\ast$} becomes optimal if the heuristic is consistent. \begin{itemize}
    \item Consistent heuristics are a subset of the admissible heuristics
\end{itemize}

~\\ \pstart
\term{Iterative Deepening A$^\ast$}: IDS, but coupled with a heuristic \begin{itemize}
    \item \textbf{Issue}: if we bound our solution depth by the distance of a path, it might occur that a single increment only gives one more possible path $\implies$ requires lots of incrementing

    \textbf{Consequence}: IDA$^\ast$ works best on graphs where incrementing the limit causes many more nodes to be added to the graph
\end{itemize}

    

\subsection{Heuristics}
One metric for the quality of a heuristic is the \term{effective branching factor} $b^\ast$, used to describe the \# of actions removed from consideration under the heuristic. \begin{itemize}
    \item A smaller effective branching factor means the algorithm takes a more direct path to goal
    \item The total \# nodes generated up to a level $d$ is given by $N=1+b^\ast+\hdots+(b^{\ast})^d$  
\end{itemize}

~\\ \pstart
Ideally, we want our heuristic to always be as high as possible, while still remaining admissible. \begin{itemize}
    \item Intuition: $A^\ast$ will explore only nodes with cost less than the goal, and will explore all such nodes; to reduce the number of nodes explored, we want our heuristic to push as many nodes above that threshold as possible
\end{itemize}


% Comparing Heuristics
\subsubsection{Comparing Heuristics}
A heuristic $h_1$ \term{dominates} another heuristic $h_2$ if $h_1\geq h_2$ for all nodes $n$. \begin{itemize}
    \item Ideally, want heuristics that are high (dominate many others), but are also also computationally cheap
    \item A \textit{perfect heuristic} is a heuristic that, given a node $n$, always gives the exact cost from that node to the goal. \begin{itemize}
        \item A perfect heuristic will explore only the nodes on the solution path; dominates all other heuristics
        \item Ex: Running $A^\ast$ as a heuristic
    \end{itemize}
\end{itemize}

\pstart
When choosing between two heuristics $h_1,h_2$ that do not dominate each other, choice of heuristic depends on priority: \begin{itemize}
    \item Fast: Take whichever one computes faster
    \item Efficient: Take \ul{$h_3=\max\{h_1,h_2\}$} as the heuristic \begin{itemize}
        \item Automatically dominates both $h_1,h_2$
    \end{itemize}
\end{itemize}

\newp
\textbf{($\ast$) Sliding Tile Heuristics}
\vspace{6pt}\newp
Three heuristics for sliding-tile: \begin{enumerate}
    \setcounter{enumi}{-1}
    \item $h_0$: $h_0(n)=0\;\forall\;n$ (trivial heuristic)
    \item $h_1$: \# of tiles in incorrect position \begin{itemize}
        \item \textit{Is admissible}: each move moves at most one tile $\to$ at least that many moves needed to put every tile in right position
    \end{itemize}
    \item $h_2$: sum of lengths of shortest paths from all tiles to their goal positions \begin{itemize}
        \item \textit{Is admissible}: Via similar argument
    \end{itemize}
\end{enumerate}

\vspace{5pt}
\begin{observation}
    $h_1\leq h_2$ for all nodes $n$. ($h_2$ dominates $h_1$).
\end{observation}

\iffalse
~\\ \titleul{Effective Branching Factors}:
\begin{enumerate}
    \item $h_0$: 2.78
    \item $h_1$: $\sim$1.48
    \item $h_2$: $\sim$1.26
\end{enumerate}
\fi


% Finding Heuristics
\subsubsection{Finding Heuristics}
Methods for finding heuristics: \begin{itemize}
    \item \term{Domain knowledge}: For some problems, there may be some property of the problem or problem solutions that can help the search algorithm find a solution faster
    \item \term{Relaxation}: Come up with game that is easier to solve, and solve it \begin{itemize}
        \item If a game has a number of rules, can loosen or delete some of them to obtain an easier game
        \item resulting heuristics are admissible - any solution for the original game still solves the simplified one, just not vice versa
    \end{itemize}
    \item \term{Subproblems}: Change the goal state to make the goal easier, and solve \begin{itemize}
        \item admissibility - solutions to real problem are subset of solutions to relaxed problem/subproblem
    \end{itemize}
\end{itemize}

\newp
\titleul{($\ast$) Ex: Sliding-Tile (\textit{Relaxation, Subproblems})}

\vspace{8pt} \pstart
First, look at the initial rules for sliding tile: \begin{enumerate}
    \item One piece can move per turn
    \item A piece A can move to a square B if: \begin{enumerate}
        \item A is adjacent to B
        \item B is blank
    \end{enumerate}
\end{enumerate}

\vspace{5pt} \pstart
Making easier games: \begin{enumerate}
    \item Delete 2(a), 2(b): \# moves to solve is \# tiles in wrong location (same as $h_1$)
    \item Delete 2(b): $h_2$
\end{enumerate}

\vspace{5pt} \pstart
Alternatively, define a \term{subproblem} with a revised goal: only certain tiles (e.g. 1, 2, 3) need to be in the right position.

\newp
\titleul{($\ast$) Ex: Traveling Salesman (\textit{Relaxation, Domain Knowledge})}
\vspace{6pt}\newp
\textit{Insight}: A solution to the traveling salesman problem is a spanning tree, plus one edge

\vspace{6pt} \pstart
$\implies$ A minimum spanning tree minus its smallest edge is smaller than an optimal solution to traveling salesman. (In particular: finding an MST is a \textit{relaxation} of traveling salesman.)

\vspace{2pt}\newp
\textit{Heuristic}: Merge all traversed edges, find cost of MST on resulting graph.


% Patterned Databases
\subsubsection{($\ast$) Patterned Databases}
\begin{observation}
    A subproblem-based heuristic is running a search inside of a search; doing this naively may accidentally repeat searches (incurring additional cost).
\end{observation}

\vspace{6pt}\pstart
\textbf{Improvement}: Rather than resolving a subproblem at each step, pre-compute a \term{patterned database} containing solutions to all possible subproblems. \begin{itemize}
    \item Generating a pattern database: generate all possible subproblems, solve them, compute solution costs, and store in some data structure. \begin{itemize}
        \item Heuristic becomes a simple  lookup once patterned database is built
        \item Combining patterned databases: Take a max
    \end{itemize}
    \item Is slow to pre-compute (same time as solving from scratch), but leads to large speedups once database is built \begin{itemize}
        \item Can solve large problems where regular heuristics fail
    \end{itemize}
\end{itemize}

\vspace{6pt} \pstart
\textit{Alternative approach}: From goal state, go backwards - compute the cost of reaching all possible initial states (from goal state) and use this cost as the heuristic. 

\vspace{8pt} \pstart
($\ast$) To combine two patterned databases, just use their max as the heuristic.

\vspace{12pt}\pstart
For certain problems, can take a partition of the original problem (into subproblems) to create \term{disjoint patterned databases}; want to be able to ``add up'' heuristics from each database to create a new heuristic. 

\vspace{8pt}
\pstart
($\ast$) Issue: adding normally might overcount actions (is not admissible); can solve by changing how actions are counted within subproblems \begin{itemize}
    \item[($\ast$)] Ex: For a sliding tile subproblem (only placing 1, 2, 3 in correct locations, e.g.), only count the actions that move a 1, 2, or 3
\end{itemize}


\end{document}
